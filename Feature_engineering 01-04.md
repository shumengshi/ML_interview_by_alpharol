### 百面机器学习 —— 第一章 特征工程  01-04

<br/>

#### 01  特征归一化

**问：什么样的变量需要特征归一化？**

答：数值型变量，类别型变量是不需要的

**问：为什么要特征归一化？**

答：首先，为了消除量纲对数据分析的影响。比如当我们衡量一个人的身高体重对健康的影响且体重选择kg作为单位时，身高单位选择cm或者m就有很大的差别。因为结果会倾向于数据分散程度（方差）比较大的部分。当我们选择cm作为单位时，身高的影响程度大；当我们选择m做单位时，体重的影响程度大。

&#160; &#160; &#160; &#160; 其次，数据归一化后，在使用梯度下降法时求解的速度会更快。实际应用中，使用梯度下降法求解的一般都需要对数据进行归一化，比如线性回归，逻辑回归，神经网络等等。但是像决策树算法一般就不需要数据归一化，因为他一般都会把数据分类变成类别型变量。

**问：特征归一化的方法**

答：线性函数归一化(Min-Max Scaling)：对原始数据进行线性变换，将其映射到$[0,1]$区间内。

$$X_{norm} = \dfrac{X-X_{min}}{X_{max}-X_{min}}$$

&#160; &#160; &#160; &#160;零均值归一法：将原始数据映射到均值为0，标准差为1的分布上。即如果数据的均值为$\mu$，标准差为$\sigma$，那么归一化的公式为：

$$z = \dfrac{x-\mu}{\sigma}​$$



<br/>

#### 02 类别型变量

&#160; &#160; &#160; &#160;类别型变量的输入一般是字符串的形式，除了决策树外一般没有模型可以处理这种输入。需要转换成数值型变量。转换的方式有如下三种：

**序号编码：** 适用于数据之间有大小关系的特征。比如成绩分为A>B>C，我们可以使用3>2>1来代表；

**独热编码：** 英文One-hot，适用于数据之间没有大小关系的特征。比如血型A/B/AB/O。编码形式：



| 血型 | 独热编码   |
| ---- | ---------- |
| A    | 1  0  0  0 |
| B    | 0  1  0  0 |
| AB   | 0  0  1  0 |
| O    | 0  0  0  1 |

&#160; &#160; &#160; &#160;需要注意的是当分类较多的时候，独热编码是非常稀疏的，可以使用下面三种方式处理：

1. 采用稀疏向量来节省空间
2. 是否可以进行特征选择
3. 使用降维的方法

**二进制编码：** 首先给每个类别赋予一个ID，然后将类别ID对应的二进制编码作为结果。本质上是使用了二进制对ID进行了哈希映射。这样得到的向量的维数将明显小于独热编码。

| 血型 | ID   | 二进制编码 | 独热编码   |
| ---- | ---- | ---------- | ---------- |
| A    | 1    | 0  0  1    | 1  0  0  0 |
| B    | 2    | 0  1  0    | 0  1  0  0 |
| AB   | 3    | 0  1  1    | 0  0  1  0 |
| O    | 4    | 1  0  0    | 0  0  0  1 |



<br/>

#### 03 高维组合特征的处理

&#160; &#160; &#160; &#160;为了提高复杂模型的拟合能力，在我们进行特征工程（数据预处理）的时候，经常把两个**离散**的特征组合到一起。这样做的目的其实是为了更加详细的分类。我们简单举例（电影语言和类型对是否观看的影响）：

| 是否观看 | 电影语言 | 电影类型 |
| :------: | :------: | :------: |
|    0     |   中文   |   动作   |
|    1     |   中文   |   喜剧   |
|    1     |   英文   |   喜剧   |
|    0     |   英文   |   动作   |

&#160; &#160; &#160; &#160;在这张表格中，我们单独拿出电影的语言或者单独拿出电影的类型都是不够准确的，所以我们考虑是否可以使用组合变量。

| 是否点击 | 中文+动作 | 中文+喜剧 | 英文+动作 | 英文+喜剧 |
| :------: | :-------: | :-------: | :-------: | :-------: |
|    0     |     1     |     0     |     1     |     0     |
|    1     |     0     |     1     |     0     |     1     |

&#160; &#160; &#160; &#160;使用组合特征这种方法在处理类别型变量时其实是很又用处的，但是这里存在这样一个问题：

&#160; &#160; &#160; &#160;当我们的数据特征A有两个分类，特征B叶有两个分类的时候，组合起来我们有2*2=4个特征；

&#160; &#160; &#160; &#160;但是，当我们数据的特征是编号的时候（这一点在社交网络分析的时候很常见），如下图：

| 是否购买 | 用户ID | 物品ID |
| :------: | :----: | :----: |
|    1     |   1    |   1    |
|    0     |   2    |   1    |
|   ...    |  ...   |   1    |
|    1     |   m    |   1    |
|    1     |   1    |   2    |
|    0     |   2    |   2    |
|   ...    |  ...   |   2    |
|    0     |   m    |   2    |
|   ...    |  ...   |  ...   |
|    1     |   m    |   n    |

&#160; &#160; &#160; &#160;假设我们有m个用户，n个物品，这里如果组合特征的话，那么就有$m*n$个特征，当m和n都很大的时候，这里需要学习的参数模型就太大了。

&#160; &#160; &#160; &#160;一种处理的方法是找到一个低维(k维)向量去代表m维和n维原始向量。那么这样参数的规模就变成了$m*k+n*k$。这里也就是说在社交网络分析中，可以使用矩阵分解的思想。

<br/>

#### 04 如何找出组合特征

&#160; &#160; &#160; &#160;一般来说，组合特征的数量级都是$m*n$级别的，即便m和n都不大，那把他们都放在模型里也是很困难的，这里我们需要一个方法找出有效的特征。一般选择的方法都是决策树，可以说树模型天生就适合处理分类变量。如果在决策树从根节点到一个叶节点的路径上出现了变量$A_1$和$B_2$，那么$A_1B_2$就是一个组合的方式。

&#160; &#160; &#160; &#160;在决策树的构造方面我们一般都会选择梯度提升决策树。